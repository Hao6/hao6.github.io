---
layout:     post
title:      线性模型初识
subtitle:   常见线性模型的原理与公式推导
date:       2019-04-27
author:     hao6
header-img: img/post5_bg_linearmodel.png
catalog: true
tags:
    - Machine Learning
---

# 线性模型初识

- 通俗来说，线性模型即是用线性组合的方式表示属性(自变量，X)与标签(因变量，Y)的关系，可以用来实现分类与回归任务

在线性模型的例子中，使用如下符号：

| num  | sign             | ps                           |
| ---- | ---------------- | ---------------------------- |
| 0    | $X_{m\times n}$  | 属性数据，m条n维属性         |
| 2    | $X^i$            | 第$i$行数据                  |
| 3    | $X_i$            | 第$i$列数据                  |
| 4    | $y_{m\times 1}$  | 标签数据，                   |
| 5    | $w_{1 \times n}$ | 权重矩阵，n个维度对应n个权重 |

## 线性回归

用来解决回归问题，其模型如下：
$$
\hat{y} = Xw^{T}
$$
代价函数(Cost function)，平方误差：
$$
J_{(w)} = \|X w^T-y\|_{2}^{2}
$$
解决线性回归问题相当于如下的最优化问题：
$$
\min _{w}\|X w^T-y\|_{2}^{2}
$$
上述优化问题的解决有两种方案，直接法与迭代法：

1. 直接法，对$w$求一次偏导数，令偏导数取值为0的$w$即为所求；
2. 迭代法，梯度下降法等，因为该最优化问题是凸优化，所以梯度下降可以得到最优解；

### 直接法

为了在求偏导数过程中表达方便，首先给代价函数乘以一个正的非零项$\frac{1}{2m}$，在不影响最优解的前提下，代价函数可以修改为如下形式：


$$
\begin{align}
J_{(w)} &= \frac{1}{2m}\|X w-y\|_{2}^{2} \\
		&= \frac{1}{2m}\sum_{i=1}^{m}(X^iw - y^i)^2\\
		&= \frac{1}{2m}((X^1w - y^1)^2 + (X^2w - y^2)^2 + \dots + (X^mw - y^m)^2)\\
		&= \frac{1}{2m}((X^1_1w_1 +X^1_2w_2+ \dots +X^1_nw_n  - y^1)^2 + (X^2_1w_1 +X^2_2w_2+ \dots +X^2_nw_n  - y^2)^2 + \dots \\
		&+ (X^m_1w_1 +X^m_2w_2+ \dots +X^m_nw_n  - y^m)^2)
\end{align}
$$

则对上述代价函数，对$w_1$求偏导数得：
$$
\begin{align}
\frac{\partial J_{(w)}}{\partial w_{1}} &= \frac{1}{m}((X^1_1w_1 +X^1_2w_2+ \dots +X^1_nw_n  - y^1)X^1_1 + (X^2_1w_1 +X^2_2w_2+ \dots +X^2_nw_n  - y^2)X^2_1 + \dots \\
										&(X^m_1w_1 +X^m_2w_2+ \dots +X^m_nw_n  - y^2)X^m_1)\\
										&=\frac{1}{m}\sum_{i=1}^{m}(Xw^{T}-y)X_{1}^i\\
										&=\frac{1}{m}\sum_{i=1}^{m}(\hat{y}-y)X_{1}^i
\end{align}
$$

同理，对于$w_i​$而言，代价函数对其偏导数为：
$$
\frac{\partial J_{(w)}}{\partial w_{1}} = \frac{1}{m}\sum_{i=1}^{m}(Xw^{T}-y)X_{i}
$$
将上述偏导数向量化之后，得到：
$$
\frac{\partial J_{(w)}}{\partial w }= \frac{1}{m}\sum_{i=1}^{m}(Xw^{T}-y)X
$$
严格来说，上面这个公式是有歧义的，实际上$(Xw^{T}-y)$的规模是$m \times 1$，而$X$的规模为$m \times n$，$\sum_{i=1}^{m}(Xw^{T}-y)X$实际上是前者作为一列，分别与后者的每一列分别做了内积操作，得到一个规模为$n \times 1$的矩阵，这在`numpy`可由广播机制实现，实际上上述公式可以改造为准确地矩阵相乘的形式。

令偏导数$\frac{\partial J_{(w)}}{\partial w } = 0 ​$，等价于如下齐次线性方程组求解问题：
$$
\begin{align}
&\sum_{i=1}^{m}(Xw^{T}-y)X = 0 \\
&\Leftrightarrow X^T(Xw^T-y) =  [0, 0, 0, \dots,0]\\
&\Leftrightarrow X^TXw^T - X^Ty = [0, 0, 0, \dots,0]\\
&\Leftrightarrow X^TXw^T = X^Ty \\
&\Leftrightarrow w^T = (X^TX)^{-1}X^Ty
\end{align}
$$
上述解称之为解析解或者闭式解，不幸的是，这个解析解的存在是有条件的，从形式上来看，$(X^TX)^{-1}​$必须存在，但并不是所有的$(X^TX)​$都可逆，当$(X^TX)​$不可逆的时候，这个解析解就不存在。当解析解不存在时，就只能使用迭代法来解决这个凸优化问题。

### 迭代法(间接法)

包括梯度下降、牛顿法等优化算法，在这里介绍梯度下降法，使用代价函数对$w​$的一阶偏导数，由于一阶偏导数在上面**直接法**部分已经求得，则迭代步骤如下：

1. 设置迭代次数***epochs***，学习率$\alpha$；
2. 当前迭代系数不超过***epochs***时，执行如下权重更新，否则返回$w​$，退出程序；

$$
w^* = w - \alpha \frac{\partial J_{(w)}}{\partial w }
$$

一般来说，当$X^TX​$规模较大时，直接法会消耗相当多的时间(求逆操作的时间复杂度为$O(n^3)​$)，应该使用迭代法求解，当$X^TX​$规模较小时，可以使用直接法求解，前提是该矩阵的逆存在。

## Lasso(套索回归)

相当于加了L1正则的线性回归，关于L1正则的作用，简单来说是能够通过使权重稀疏化，即权重中的一些项最终收敛为0来降低线性模型的过拟合程度。关于为什么L1正则能够使权重稀疏化，则有两种一表一里的解释，先暂时按下不表。其模型与线性回归模型一致：
$$
\hat{y} = Xw^{T}
$$
其代价函数正如上述所言，添加了L1正则项，如下：
$$
\begin{align}
J_{(w)} &= \frac{1}{2 m}\|X w^T-y\|_{2}^{2}+\lambda\|w\|_{1} \\
		&= \frac{1}{2 m}\sum_{i=1}^{m}((X^iw^T -y^i)^2) + \lambda\|w\|_{1}
\end{align}
$$


则解决Lasso就相当于解决如下最优化问题：
$$
\min J_{w} \Leftrightarrow \min \frac{1}{2 m}\sum_{i=1}^{m}((X^iw^T -y^i)^2) + \lambda\|w\|_{1}
$$
由于Lasso存在闭式解的条件非常苛刻，所以直接介绍使用迭代法来进行最优化，由于加入L1正则化项，破坏了凸优化的条件，所以不能直接采用梯度下降法来进行优化，而是采用角度下降法(Coordinate Descent)或这最小角回归(LARS)的方法来进行优化，首先介绍角度下降法。

### 角度下降法

角度下降法，依次选择某一权重，在固定其他权重的基础上进行最小值优化，在某一权重上的最小值优化可采用梯度下降法等迭代算法，下面以梯度下降为例，角度下降法的步骤为：

1. 确定迭代次数***epochs***，学习率$\alpha$，正则化系数$\lambda$，精度$\epsilon​$，维度数量**n**；
2. 若当前维度k<=n，转3，否则返回$w$，退出程序；
3. 当前迭代系数不超过***epochs***时或者两次权重更新之间的代价函数超过$\epsilon$，执行如下权重更新，否则转2：

$$
w^*_k = w_k - \alpha \frac{\partial J_{(w)}}{\partial w_k }
$$

### 最小角回归法

最小角回归(LARS)的方法实际上是与最小角回归的优化算法有关，在搞懂最小角回归之后，我们再回来讨论使用最小角回归的优化算法来最优化Lasso的代价函数。

## Ridge(岭回归)

相当于加了L2正则的线性回归，关于L2正则的作用，简单来说是能够通过使权重减小，来降低线性模型的过拟合程度。关于为什么L1正则能够使权重稀疏化，则有两种一表一里的解释，先暂时按下不表。其模型与线性回归模型一致：
$$
\hat{y} = Xw^{T}
$$
其代价函数正如上述所言，添加了L1正则项，如下：
$$
J_{w}=\|X w-y\|_{2}^{2}+\lambda\|w\|_{2}^{2}
$$
为了方便表达求一阶偏导数之后的表达式，可以修改为：
$$
J_{w}=\frac{1}{2m}\|X w-y\|_{2}^{2}+\frac{\lambda}{2}\|w\|_{2}^{2}
$$
则根据线性回归的偏导数求解结果，可以轻松得到该代价函数对$w$的一阶偏导数，如下：
$$
\frac{\partial J_{(w)}}{\partial w }= (\frac{1}{m}\sum_{i=1}^{m}(Xw^{T}-y)X) + \lambda w
$$
同线性回归，Ridge存在直接法与迭代法两种解法；

### 直接法

令偏导数$\frac{\partial J_{(w)}}{\partial w } = 0 $，等价于如下齐次线性方程组求解问题：
$$
\begin{align}&(\sum_{i=1}^{m}(Xw^{T}-y)X)+ \lambda w = 0 \\
&\Leftrightarrow X^T(Xw^T-y) + \lambda w^T =  [0, 0, 0, \dots,0]\\
&\Leftrightarrow X^TXw^T - X^Ty + \lambda w^T = [0, 0, 0, \dots,0]\\
&\Leftrightarrow X^TXw^T + \lambda w^T = X^Ty \\
&\Leftrightarrow (X^TX+\lambda E)w^T= X^Ty \\
&\Leftrightarrow w^T = (X^TX+\lambda E)^{-1}X^Ty
\end{align}
$$
其中$E$指单位矩阵，则**$\lambda E$**形象的看做一条山岭(Ridge)，这也是Ridge(岭回归)名称的来源。

### 迭代法

包括梯度下降、牛顿法等优化算法，在这里介绍梯度下降法，使用代价函数对$w$的一阶偏导数，由于一阶偏导数在上面**直接法**部分已经求得，则迭代步骤如下：

1. 设置迭代次数***epochs***，学习率$\alpha​$；
2. 当前迭代系数不超过***epochs***时，执行如下权重更新，否则返回$w$，退出程序；

$$
w^* = w - \alpha \frac{\partial J_{(w)}}{\partial w }
$$

一般来说，当$X^TX$规模较大时，直接法会消耗相当多的时间(求逆操作的时间复杂度为$O(n^3)$)，应该使用迭代法求解，当$X^TX$规模较小时，可以使用直接法求解。

## Elastic Net(弹性网)

是一种结合了L1正则与L2正则的回归算法，其模型与线性回归模型一致：
$$
\hat{y} = Xw^{T}
$$
其代价函数为：
$$
J_{w} = \frac{1}{2m}\|X w-y\|_{2}^{2}+\lambda \rho \|w\|_{1}+\frac{\lambda(1-\rho)}{2}\|w\|_{2}^{2}
$$
则求解、训练Elastic Net相当于最小化该代价函数：
$$
\min J_{w} \Leftrightarrow \frac{1}{2m}\|X w-y\|_{2}^{2}+\lambda \rho \|w\|_{1}+\frac{\lambda(1-\rho)}{2}\|w\|_{2}^{2}
$$
因为L1正则项的存在，我们采用角度下降法来进行优化。

## Logistics(逻辑斯底回归)

这个东东其实是个分类器，准确来说是二元分类器，标签数据是离散而不是连续的。线性回归的结果是分布在一定区间上的连续值，为了实现分类任务，直观的想法是将这些连续值通过某种函数映射到[0,1]上解释为概率，即映射结果为成为两类中某一类的概率，一般来说是成为正类的概率，若映射结果>0.5，则认为该样本被预测为正类(记为1)，反之则预测为负类(记为0)。Logistics呢，实际上基于上面描述的映射函数有关，Logistics(逻辑斯底回归)使用如下称之为**logit**的函数作为映射函数：
$$
sigmoid(x) = \frac{1}{1 + \exp{(-x)}}
$$
![](https://picgo-image-hosting.oss-cn-beijing.aliyuncs.com/img/sigmoid.png)

**logit**函数具有一些好的性质是入选映射函数的必需条件。首先，该函数将一切实数都映射到了(0, 1)上，这对结果输出提供了很方便的解释；其次，该函数处处连续可导，在使用迭代法时便于求偏导数，即满足迭代法的要求。

下面是Logistics(逻辑斯底回归)模型的表达式，与线性回归相比，多了一步映射步骤：
$$
\begin{align}
\hat{y} &= sigmoid(Xw^{T})\\
		&= \frac{1}{1+\exp(-Xw^T)}
\end{align}
$$
其中，将预测值$\hat{y}$解释为预测结果为正类的概率。

下面部分是关于Logistics(逻辑斯底回归)损失函数的讨论，一般来说对于参数模型，都是定义一个代价函数，然后最优化代价函数来达到训练模型的目的的。Logistics(逻辑斯底回归)没有使用前面几种线性模型所使用的平方误差函数，而是使用了交叉熵损失函数。关于为什么使用交叉熵损失函数，有几个方面的考量。除此之外，还可以使用极大似然法(MAP)来得到与使用交叉熵损失函数后一样的最优化问题。下面先给出Logistics(逻辑斯底回归)代价函数的定义。需要说明的是，有关损失函数(loss function)与代价函数(cost function)的区别，损失函数是针对一个样本而言，衡量预测结果与真实标签之间的差异，代价函数是对样本整体而言，所有样本损失之和或者平均值。因此定义了损失函数，也即定义了代价函数。Logistics(逻辑斯底回归)的损失函数(交叉熵损失函数)与代价函数如下：
$$
\begin{align}
loss_{w} &= -y\ln{\hat{y}} - (1-y)\ln{(1-\hat{y})} \\
		 &= -y\ln(1/(1+\exp(-Xw^T))) - (1-y)\ln(\exp(-Xw^T)/(1+\exp(-Xw^T)))
\end{align}
$$

$$
\begin{align}
J_{w} &= \sum_{i=1}^{m}loss_w\\
		 &= -\sum_{i=1}^{m}(y\ln{\hat{y}} + (1-y)\ln{(1-\hat{y})})\\
		 &= -\sum_{i=1}^{m}(y\ln(1/(1+\exp(-Xw^T))) + (1-y)\ln(\exp(-Xw^T)/(1+\exp(-Xw^T))))
\end{align}
$$

则Logistics(逻辑斯底回归)模型的训练可转化为解决如下最优化问题：
$$
\min J_{w} \Leftrightarrow \min (-\sum_{i=1}^{m}(y\ln{\hat{y}} + (1-y)\ln{(1-\hat{y})}))
$$
为什么要使用交叉熵损失函数而不是平方误差损失函数呢？现在来比较一下交叉熵损失函数在的Logistics(逻辑斯底回归)问题上的优势。

![](https://picgo-image-hosting.oss-cn-beijing.aliyuncs.com/img/se_loss.png)

上图是平方误差损失函数，真实标签为1与为0的图像是对称的，所以只使用*t=1*的情况来做说明，当真实标签为1时，随着x取值变大，sigmoid(x)趋向于1，所以损失趋向于0，而x取值变小，sigmoid(x)趋向于0，所以损失趋向于0.5，在这一点上平方误差损失函数是合格的。但是考虑这种情况，x1 = -7 与 x2 = -8，sigmoid(-7)与sigmoid(-8)非常接近，从而导致在这两种情况下的损失非常接近，但是在直觉上这是不可靠的，即在一定犯错误的尺度之内，两者的损失非常接近，这种反直觉的效应反映到上图中，就是当x取值非常小且*t=1*的情况下，其斜率(导数值)非常小，进一步导致了在梯度下降时收敛的非常慢，按照正常的直觉，分类器犯得错误越大，其损失越大，且其在该点应该收敛的更快(在学习率固定的情况下，梯度更大)。而平方误差函数是不满足(违背)这一要素的，所以Logistics(逻辑斯底回归)使用下图所示的交叉熵损失函数。

![](https://picgo-image-hosting.oss-cn-beijing.aliyuncs.com/img/ce_loss.png)

根据上文中的分析，交叉熵损失函数正好符合这种直觉，在图像上也能反映出来。

此外，还可以从**极大似然法(MAP)**角度出发来得到如上的最优化问题，将$\hat{y}$解释为预测样本为正类的概率，则其为负类的概率为$1-\hat{y}$，即如下所示：
$$
P{(y^i|X^i,w)}=\left\{
\begin{array}{rcl}
\hat{y}           &      & {y^i = 1}\\
1 - \hat{y}       &      & {y^i = 0}
\end{array} \right.
$$
可将上式合并为一个式子：
$$
P{(y^i|X^i,w)} = y^i\hat{y}^i+(1-y^i)(1-\hat{y}^i)
$$
则根据极大似然法(MAP)，优化下面的函数即可求得$w$：
$$
\max\prod_{i=1}^{m}P{(y^i|X^i,w)} \Leftrightarrow \max\prod_{i=1}^{m}y^i\hat{y}^i+(1-y^i)(1-\hat{y}^i)
$$
使用对数似然函数处理上述优化问题，如下：
$$
\begin{align}
&\max\prod_{i=1}^{m}P{(y^i|X^i,w)}\\
\Leftrightarrow & \max(\ln(\prod_{i=1}^{m}P{(y^i|X^i,w)})\\
\Leftrightarrow & \max\sum_{i=1}^{m}\ln(P{(y^i|X^i,w)})\\
\Leftrightarrow & \max\sum_{i=1}^{m}\ln(y^i\hat{y}^i+(1-y^i)(1-\hat{y}^i))\\
\Leftrightarrow & \max\sum_{i=1}^{m}(y^i\ln{\hat{y}^i} + (1-y^i)\ln{(1-\hat{y}^i)})\\
\Leftrightarrow & \min -\sum_{i=1}^{m}(y^i\ln{\hat{y}^i} + (1-y^i)\ln{(1-\hat{y}^i)})
\end{align}
$$
现在得到了与上文给出的一样的最优化问题，即从极大似然法的角度也能得到使用交叉熵损失函数的最优化问题。

既然现在得到了上述的最优化问题，那么接下来就是使用迭代法(这里使用梯度下降法)来解决最优化问题的时候了(因为这是个凸优化问题，所以梯度下降是可以得到全局最优解的)，使用迭代法的关键是求解问题的偏导数，下面来使用链式法则推导来推导上述优化问题的偏导数，你会对结果感到吃惊的(数学之美)。
$$
\begin{align}
J_{w} &= -\sum_{i=1}^{m}(y^i\ln{\hat{y}^i} + (1-y^i)\ln{(1-\hat{y}^i)}) \\
      &= -(y^1\ln\hat{y}^1 + (1-y^1)\ln(1-\hat{y}^1) + y^2\ln\hat{y}^2 + (1-y^2)\ln(1-\hat{y}^2) + \dots \\
      & +y^m\ln\hat{y}^1m +(1-y^m)\ln(1-\hat{y}^m))
\end{align}
$$

$$
\begin{align}
\frac{\partial J_{(w)}}{\partial w_{1}} &= -(y^1\frac{1}{\hat{y}^1}\frac{\partial \hat{y}^1_{(w)}}{\partial w_{1}}+(1-y^1)(\frac{-1}{1-\hat{y}^1})\frac{\partial \hat{y}^1_{(w)}}{\partial w_{1}} + y^2\frac{1}{\hat{y}^2}\frac{\partial \hat{y}^2_{(w)}}{\partial w_{1}}+(1-y^2)(\frac{-1}{1-\hat{y}^2})\frac{\partial \hat{y}^2_{(w)}}{\partial w_{1}} + \dots \\
&+y^m\frac{1}{\hat{y}^m}\frac{\partial \hat{y}^m_{(w)}}{\partial w_{1}}+(1-y^m)(\frac{-1}{1-\hat{y}^m})\frac{\partial \hat{y}^m_{(w)}}{\partial w_{1}})\\                 
&= -(\frac{y^1}{\hat{y}^1}\frac{\partial \hat{y}^1_{(w)}}{\partial w_{1}}+(\frac{(y^1-1)}{1-\hat{y}^1})\frac{\partial \hat{y}^1_{(w)}}{\partial w_{1}} +\frac{y^2}{\hat{y}^2}\frac{\partial \hat{y}^2_{(w)}}{\partial w_{1}}+(\frac{(y^2-1)}{1-\hat{y}^2})\frac{\partial \hat{y}^2_{(w)}}{\partial w_{1}}+\dots\\
&+\frac{y^m}{\hat{y}^m}\frac{\partial \hat{y}^m_{(w)}}{\partial w_{1}}+(\frac{(y^m-1)}{1-\hat{y}^m})\frac{\partial \hat{y}^m_{(w)}}{\partial w_{1}})
\end{align}
$$

其中，
$$
\hat{y}^i = \frac{1}{1+\exp(-X^iw^T)}
$$
则：
$$
\begin{align}
\frac{\partial \hat{y}^i_{(w)}}{\partial w_{1}} &= \hat{y}^i(\hat{y}^i -1)\frac{\partial (-X^iw^T)}{\partial w_{1}}\\
&=\hat{y}^i(\hat{y}^i-1)(-X^i_1) \\
&=\hat{y}^i(1-\hat{y}^i)X^i_1
\end{align}
$$
将其代入上面代价函数对$w1$求偏导数的式子中，得：
$$
\begin{align}
\frac{\partial J_{(w)}}{\partial w_{1}} &= -(\frac{y^1}{\hat{y}^1}\frac{\partial \hat{y}^1_{(w)}}{\partial w_{1}}+(\frac{(y^1-1)}{1-\hat{y}^1})\frac{\partial \hat{y}^1_{(w)}}{\partial w_{1}} +\frac{y^2}{\hat{y}^2}\frac{\partial \hat{y}^2_{(w)}}{\partial w_{1}}+(\frac{(y^2-1)}{1-\hat{y}^2})\frac{\partial \hat{y}^2_{(w)}}{\partial w_{1}}+\dots\\
&+\frac{y^m}{\hat{y}^m}\frac{\partial \hat{y}^m_{(w)}}{\partial w_{1}}+(\frac{(y^m-1)}{1-\hat{y}^m})\frac{\partial \hat{y}^m_{(w)}}{\partial w_{1}})\\
&=-(\frac{y^1}{\hat{y}^1}\hat{y}^1(1-\hat{y}^1)X^1_1+(\frac{(y^1-1)}{1-\hat{y}^1})\hat{y}^1(1-\hat{y}^1)X^1_1 +\frac{y^2}{\hat{y}^2}\hat{y}^2(1-\hat{y}^2)X^2_1+(\frac{(y^2-1)}{1-\hat{y}^2})\hat{y}^2(1-\hat{y}^2)X^2_1+\dots\\
&+\frac{y^m}{\hat{y}^m}\hat{y}^m(1-\hat{y}^m)X^m_1+(\frac{(y^m-1)}{1-\hat{y}^m})\hat{y}^m(1-\hat{y}^m)X^m_1) \\
&=-(y^1(1-\hat{y}^1)X^1_1+(y^1-1)\hat{y}^1 X^1_1 + y^2(1-\hat{y}^2)X^2_1+(y^2-1)\hat{y}^2 X^2_1+\dots \\
&+y^m(1-\hat{y}^m)X^m_1+(y^m-1)\hat{y}^m X^m_1)\\
&= -((y^1-\hat{y}^1)X^1_1 + (y^2-\hat{y}^2)X^2_1 + \dots + (y^m-\hat{y}^m)X^m_1)\\
&= (\hat{y}^1 - y^1)X^1_1 + (\hat{y}^2 -y^2)X^2_1 + \dots + (\hat{y}^m-y^m)X^m_1\\
&= \sum_{i=1}^{m}(\hat{y} - y)X_1^i
\end{align}
$$
对比上式与线性回归的代价函数对$w_1$的偏导数的表达式，可以发现两个式子只是差了一个常数$\frac{1}{m}$的区别，即实际上两者在本质上是一样的(是巧合吗？数学之美)。那么，接下来，即可得到Logistics(逻辑斯底回归)偏导数的向量化形式如下：
$$
\begin{align}
\frac{\partial J_{(w)}}{\partial w } &= \sum_{i=1}^{m}(Xw^{T}-y)X \\
									 &= (Xw^{T}-y)^TX
\end{align}
$$
